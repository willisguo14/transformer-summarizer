{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VInmKSkhEhle"
   },
   "outputs": [],
   "source": [
    "#dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    \n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4rgPxYSRWQS"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding / separator\n",
    "EOS = 1 # End of sentence \n",
    "\n",
    "#concatenate inputs\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) \n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "\n",
    "input_pipeline = trax.data.Serial(\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    preprocess,\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "uKFoGsUKSa_I",
    "outputId": "bc4d6634-d716-4311-d49c-1956bca2bc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "S4uHyCkbSuUo",
    "outputId": "52845be8-f2fc-4803-bf7a-ed9725fe2bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " Fears are growing that Britain's jails are becoming a hotbed of\n",
      "extremism after it was revealed today that nearly half the inmates of\n",
      "one top security prison are Muslim. Some 42 per cent of those housed\n",
      "at Category A Whitemoor jail - and more than a quarter of those in\n",
      "London prisons - consider themselves to be of Islamic faith. Experts\n",
      "now fear large numbers are being radicalised on the inside, where they\n",
      "say the spread of Jihadist ideas is rife. Figures show more than a\n",
      "quarter of inmates in London jails are Muslim, with one Category A\n",
      "jail revealing 42 per cent of its convicts follow the Islamic faith .\n",
      "Whitemoor inmate Zia Al Haq, left, was jailed for 18 for planning bomb\n",
      "attacks in London while Nezar Hindawi, right, was handed a 45-year\n",
      "sentence for plotting to blow up a jet . A source at Cambridgeshire\n",
      "jail Whitemoor told the Sunday People: 'Whitemoor is now effectively\n",
      "run by Muslims, many of whom are Jihadis.' A 2012 probe into the jail\n",
      "branded it a 'Taliban recruiting ground' and said inmates were offered\n",
      "protection if they converted to the religion. Shadow Justice Minister\n",
      "Sadiq Khan claimed ministers are not doing enough to tackle the issue\n",
      "of radicalisation in prisons. He said: 'In jails like Whitemoor, the\n",
      "Chief Inspector is on record warning of the risks of radicalisation.\n",
      "The Government needs to wake up to his problem before it is too late.'\n",
      "Shadow Justice Secretary Sadiq Khan says too little is being done to\n",
      "tackle the problem at jails like Whitemoor . A source inside Whitemoor\n",
      "said the jail was 'effectively run' by its large Muslim population .\n",
      "Whitemoor houses terrorists including Nezar Hindawi, who was given a\n",
      "45-year sentence for trying to bomb an airliner. 'In jails like\n",
      "Whitemoor, the Chief Inspector is on record warning of the risks of\n",
      "radicalisation. The Government needs to wake up to his problem before\n",
      "it is too late' Sadiq Khan, Shadow Justice Minister . Also jailed\n",
      "there is Zia Al Haq, who has been locked up for 18 years in 2007 for\n",
      "offences including trying to bomb the London Underground. The Ministry\n",
      "of Justice insists prison wardens are 'working hard to tackle\n",
      "extremist ideologies'. A Prison Service spokesman said last night:\n",
      "'Prisoners are held in establishments most suited to managing their\n",
      "individual needs and level of risk. 'The most recent independent\n",
      "inspection of HMP Whitemoor found it to be a safe environment and\n",
      "praised staff for their professionalism and dedicated\n",
      "care.'<EOS><pad>IncreasingMuslim prison population highlighted by\n",
      "Whitemoor jail . More than one in four of its convicts say they are of\n",
      "Islamic faith . Fears the trend is down to growing radicalisation by\n",
      "Jihaists . Shadow Justice Secretary Sadiq Khan calls for government\n",
      "action .<EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqj1NsbERWQX"
   },
   "outputs": [],
   "source": [
    "# Bucketing to create batched generators.\n",
    "\n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6M5OA8QRWQb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1924)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SjNOlljxTGuQ",
    "outputId": "9227c68c-6369-4ce8-8137-506c594f6ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  202 14607  5318   861 15883  1353   452  1292 11146    76   130  8016\n",
      "    19    90   196     3   200   148   130  1188  3768   213  1483   527\n",
      "   607   678     2    90  7511  1171 19941     2   130  1188  1475    61\n",
      "    38  5799  1019   156     3    13  2492    61  7084   444   110   285\n",
      "   130   296  1793     7    26  1325   320  3273   132   566  1292   355\n",
      "   213  8866 25727     4   527  2528  1480    40    46  1471    78   350\n",
      "  1068   132    28  6294   214 22820 11214    21    76    28   238   527\n",
      "  4131 14777 24302   691   213  5071   763  4945     3   577  7511   213\n",
      "   350  1034  9423   248 24792   119  2063   132  2045     2   130  1188\n",
      "   133  1084   285    13  1353  7087  7033    47   809  2946   180   286\n",
      "   320  2040    38   150  4394  3771     3    13   540   320 14657  5849\n",
      " 15037   111   130  1188    76   186    28    60    76    41   547   213\n",
      "  1282   132    31 11261     3    52  1353   213   669 27634     4 20096\n",
      "  7086 27634   391  4394    76  4872    28   666  2908 14940    95 12111\n",
      "     7     5 25629   713   171   186   145   213  1207     2 15711 20096\n",
      " 18853     2  5805 18853   186  1380     6 22820 11214    21 23196  6095\n",
      "    76  4872    13   559   320  6587   285    54   628   790     7    26\n",
      "   107    93     3    13   790     7    26  1194 11406   809   213    55\n",
      "     2   186   103  9328    28   445     3  3700   285   194     2    13\n",
      "     7   371   369  6514   130  1188  4991   156   285   103   143  1151\n",
      "    28   209    55   171    13 11850   579   107   824   457     3    13\n",
      "  1353   635   809   213    55     3   207    25   231    29    13    62\n",
      "   382   172    97    72  1161   385   412    28   441     6   104     6\n",
      "   292     3   758  2225   379   861    60  2225   320  4557  1999     7\n",
      "     5  4907  1948 11312 13615  1353   320  2040  1057  6835   385    28\n",
      "  2439  1207     3    52   255    18    46   130 15883     7     5    60\n",
      "  2225   403     2   166    13 26423   114  2118  5709   213   927   186\n",
      "  4397    61  5053    78   213  6993 13326  3831   333     2  1480  1353\n",
      "  9567  1019 11412   101     3 25974  1407     2    51  7485   246     3\n",
      "    27   855  1622  1763    93    87  6004     2    90    51  5791     3\n",
      "    13  1353 23764  6859 20236  2458  6278   527  7270   607   824  3024\n",
      "  1353     3   129  1459    28  1622   527    91   272   320  2040   130\n",
      "    60   669 27634     4  4394 27634   391  1207    11   350  1068  2614\n",
      "     3   213   515 15331     3    56    55    51    25 20643   132   213\n",
      "  3303   219     2   132   213 13037     5 10921  8135  1538     2  1480\n",
      " 19923   918    97   390   229    28  6993 13326  3831   333     3    13\n",
      "  2118   213 11332    14   186   213 15641  2685   824   707   194     3\n",
      "    13    40   553   213 12839   283   132    31 17962    61   282   214\n",
      "  1057  6835   186    13  1669   285    41    25   579   707     3   207\n",
      "    40 23492   431  2077  9078   185   186  3329  7714  6697    26 19856\n",
      "   279     3   129   540    77   263     2   365  1248  2659   527   469\n",
      "     2   141   320  2040   213 17962  8556     3     9 15641  1353 26884\n",
      "     4     3    13   358     7    26  2118   196  2685   213   584    76\n",
      "    13  2118    44  2685   213   807     3   207    25 10178   320    93\n",
      "     3  5301    77  1353    87 23646    20   403     2   579  2685   213\n",
      " 24129  9471     5  1838   525     2   525   463    78    89 18665     5\n",
      "     3   129   149   320  1355   105   239   213  4051   527   213   927\n",
      "     2  7501    31  3827  1172  4336 16807     3   244    31 13244     5\n",
      "    76   141   403  4468     3 13548    47     7    26   172  2147   379\n",
      "    13   288    13   790     7    26   172   213  2147     3    13   358\n",
      "     7    26   362    94  2680   206    76    41    25   141 23605     5\n",
      "     3   348   213   250   527   385    51    62   655    78   213   352\n",
      "   320  1124   320  4437   105   186 22968    36    44  4336  7897     3\n",
      "  6726  4872    77   229 12467   527   156 19129    16 13783 14899 12142\n",
      "   691 20662    16    15   179     3    13  2118  4991    38   130  1078\n",
      "   285    13    40 15301   213   258  9078    81     3   639    13    40\n",
      " 10179  2685   103     2  6101   320  8819  9581 21497  2466 17406   156\n",
      "   527   213    55    51    25  1429   320   211   134   320  6700  1838\n",
      "    87  6762     7     5  7686   877     3    69 21869    21   186    51\n",
      "    38  1365   533  9739     3    13  6587   213  3146   527   285   752\n",
      "   169     3 10168    14    28   767  3454  6642  1838   213   164    49\n",
      "    98  5301    89  1188    62    18 14957  2158    64     2    35   824\n",
      "  1793     7    26  2685  2147   320    93     3  1225  6011   377  7270\n",
      " 23786    16   186  3489    41    25     3 18780   320   616  4336 16807\n",
      "   186  2378   320 11481 16488  1248   213  1917 17524     3  6382    17\n",
      "    28 14210   379    52  1353  1248  6917   658   186 17970   204   285\n",
      "    13  7034   669 27634     4  6382    17    28 14210 27634    76    28\n",
      " 14607   261  7571  4833  1480    60 25409    21   220   104     3    52\n",
      "  1003   156  1212  7098    71   213   807     2   213  1699     2   515\n",
      " 15331  5505   186  2855   130   278   296     3   244   192   669 27634\n",
      "     4  6382    17    28 14210 27634   391   229   163  1422   878  1019\n",
      "   213   480     2   103   143    18   141   412  2168    46  6280   669\n",
      " 27634     4  6382    17 11063    45  4172 27634   391   449     7     5\n",
      "  2754    41    25   320    93   763   350 19011     5     2   186    13\n",
      "   362  1577   246   213 12839   283   807  1425   285   403     3    52\n",
      "     7     5    38  2685  2754   384   527   213   314    33    25  5053\n",
      "     2    13  6847     3   252    31   955   278     2   213   515   879\n",
      "   807    25  7813 23935   691   773   186    87    25  2025   320  1013\n",
      "  5373   320   191    28   822     3     9  4584   186    54   106  8639\n",
      "     5  1480 15792  1962    87   527    31   807   456  3735 11302    21\n",
      "   809    38   527   130 10969     3    13   721  2026  2754    51   412\n",
      "   350 19011     5   143    18   757  1019   105     3 14025    51    18\n",
      "   757    44    98  6308    51   615   102   105   110   811   102   213\n",
      "   503    98    52     7     5    28  1181    36     3   326   313   790\n",
      "     7    26 16041   103     2    92  1106  2754  7019    41   133     3\n",
      "   885    49  1151 12928     4     3 18229  6602    21   691 20892   379\n",
      "    52 18561   156    61   320   362  2754   161   807 14909    21   637\n",
      "    31   955     3   207    25 25331   375   691   106 20892   132    31\n",
      "   278   628     2   186   764    13  2657    41  9737  1669  2754    41\n",
      "    40   757  1019   506  5505  1487   107   156     3    13   540   320\n",
      "   172   669 27634     4   608 27634   391   566  5505   510   809   213\n",
      "   669 27634     4  1419 27634   391   338     3    13   540   320  1179\n",
      "    82 10796  5696   595 21024     5     3    13   540   320  8234   107\n",
      " 12142     2  9660     5   805   186 14229  2956  1659     3    13   540\n",
      "   320 23127    14   804  6342     7     5   258 14994  2872  1433    78\n",
      " 24859   494  3809     2   352   107 11561   132 22740 10384 19667     4\n",
      "   186 10121     4   213 26499   186  9022   991 19570   527   213   258\n",
      "  5361 11601    14     3   244   579  1788    76    41  2920    93    19\n",
      "   320   245  7238   403  5585   691  2313  7270   196    36    49  2081\n",
      "   880    78   213  1292   352     3  1715 14712    45     2    13     7\n",
      "   371   369  6514    31 14712    45     3  1888   527    31  1303   229\n",
      " 13655 18669   132   130  2356     2   186   527    38   527   161  1779\n",
      "   980   213  3771     3   207   345   160   527   130  3241  3369   186\n",
      "   285   527  2659   527   350  1034  1487     3   397    13   790     7\n",
      "    26  1194   809   213    55     2  1353  2754   103   436  1019   161\n",
      "   313   320   962   268    78    89 18665     5     8   750  6856    24\n",
      "   207   255    18    46 13020    21    76   456 13020    21     3    13\n",
      "   288    13    62    18    46     3    34   213  2264  1248  4181   183\n",
      "  8454     2 17198  4414   527   213  1131   515 15331   248   285  5099\n",
      "   175  5505  1019    72  2007     2    22   127    92  1055   527   750\n",
      "    62    18   133   134   291    28 15412  3595     3    13    62   273\n",
      "   412   525   412   320   476    92  1055   527   750    62    18   547\n",
      "   130 14431   809 10563   320  1536   320   350  1068     3 10117   277\n",
      "   379    52     7     5    19  1019   156   320  4038     2    35   161\n",
      "   313    25 12544  8450   431     3    34    31  5505   186   132    31\n",
      "  4354  2132     2   186    13  1371   285     3    27   335    91   272\n",
      "     2    13   206  6227  7270   213   767   515   879   807   540   320\n",
      "  1536   354   669 27634     4   763 27634   391  2369   186  1558   132\n",
      "   669 27634     4   763 27634   391   370     3    13  2118  4823   130\n",
      " 15883  7270   285   806     3   305   790     7    26   288     2    35\n",
      "    33   143   172   285   130   351 23646    20  1353  1441     3    13\n",
      "  1353 15605   285    44   807   790     7    26   245    61   213  1171\n",
      "   527   144 11203  1019   213  4833     3    13  1353 20361 13203    21\n",
      "   691 21497     2   805   186  6342     7     5  5784     2 12749     5\n",
      "   186 14489   183  6363     3   198  2453    92   608 24868     2  1480\n",
      "  1353  8809   186 10553    16  1019    87  1115     3   187  1019   161\n",
      "  1779  7179   163  2264     2    31 22897  5912  1493  1577     3 21497\n",
      "  1353   141  7270    13  2118   134     3 16916 20555    93     2 24537\n",
      "  3095   186   452   996    78   213  5302   384   527   177     3   129\n",
      "   540   320   288   134   110   265     2   412    22  1459   320   385\n",
      "  1019   213  7802  2799   303     3     9   503   285    22   234    23\n",
      "    38    15 13853 16083     4   614   285   213  1699  2482   579   320\n",
      "   134     2   186   285    22  5798   160   527   425   186   425   527\n",
      "  5662   132   350  1068     3    13   358     7    26   288   122    41\n",
      "  1388    93   412    28   773    35    13     7    21   107   320   827\n",
      "    90     2   188   122   103  1353   163  4288 24245  5242 16204    21\n",
      "   186  8937  4546   773     3   244   691   824    13  1134   285   763\n",
      "   350  1068  9933    64   374  5505   927   320  2040   767   515 15331\n",
      "     2   533   278   186   133  1084   285   669 27634     4     9  8427\n",
      "   691  3794 27634   391  1353   213    94  7034   480    78  1282    76\n",
      "   186   764   767   101    25  2170   196  6503   186  9641  1838  1327\n",
      "     3  5572 25420  1044 11969 27634     4  6382    17    28 14210 27634\n",
      "   391  1606 11828  3599     3    13   107   213   503   285    33   980\n",
      "   213  1699  1838    28   350  1034  2539   412   110     3  1109   807\n",
      "   107  2483 11885     4  7534  4424     2  6777 19191   186  4181   183\n",
      " 14477    43  3528   823   177  5651     5  1019   144     2   110     2\n",
      "   350  1034     2   186   213   480  3768   285     3    13   288   285\n",
      "     2 23900     2   213  1699  1353 10148   114 20098     2    35  1019\n",
      "   163 13379  1752     6   104     6   292  9412   854   527   351 12539\n",
      "  4990  1204   186  4830     2   103  1353  1365    28  1832   320   172\n",
      "   213   175     7     5   269 23605     5 14943    93    78   213   175\n",
      "     7     5    94  1594  1026     3 23652     2    13     7    21    43\n",
      "   107   320  1029   285   350  1034 23605     5   186 18341    25  1429\n",
      "   320   344   213   266   213   138  1195     2   186   285  6220   123\n",
      "  2414  1353   498    10     1     0 24351    16  4720   320  1329   214\n",
      " 22820 11214    21   132   350  1068  1734   132  2528 16346 27439  6774\n",
      "  7583 27634     4 13414     4  8160 27634   391    25  1232  6832     2\n",
      "  8581 18750    67     2   566  3771 16346 27439  6774  1628  2076   527\n",
      "   515   879  5505   807  3198   350  1068   132  2038 16346 27439  6774\n",
      "  1628   207    25  7813 23935   691   773    78    31   955   278   320\n",
      "   213  8822  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bu05ZwbWTE6P",
    "outputId": "3d455bd7-e343-4c25-a467-572d2abd837f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " (CNN) -- My mom was always sports mad -- my dad not so much. But both\n",
      "my parents understood the importance of significant events, so when\n",
      "opportunity knocked, my parents opened up all doors for me. I grew up\n",
      "knowing full well that my country wasn't allowed to participate in\n",
      "international sports following the sporting boycott of 1977 which had\n",
      "been placed on South Africa in a protest against apartheid -- a system\n",
      "of racial segregation enforced by the ruling white minority. So when\n",
      "the South African rugby team toured New Zealand in 1981, my parents\n",
      "made sure that I was awoken at 04:30 to watch all three Test matches.\n",
      "I got to snuggle between my parents -- and a first -- they put the\n",
      "television in their bedroom. It was the \"flour bomb\" Test -- where a\n",
      "light aircraft flew over Auckland's Eden Park before and during the\n",
      "match, dropping flour bombs, smoke bombs and anti-apartheid leaflets\n",
      "-- where I began to realize that other countries didn't like us. I\n",
      "didn't understand why at the time, and it hurt a little. Later that\n",
      "day, I'll never forget my parents telling me that it could be a long\n",
      "time before I witnessed something like this again. I was six at the\n",
      "time. They were right; I would next see these two teams play as a\n",
      "17-year-old. First trip . My first trip to Cape Town's Newlands\n",
      "Cricket Ground was to watch Western Province play a domestic match. It\n",
      "must have been my mom's first trip too, because I distinctly remember\n",
      "entering the ground and ending up sitting on the grass embankment,\n",
      "which was reserved for colored people. Confused, we sat down. A kind\n",
      "couple offered us some fruit, so we stayed. I was blissfully unaware\n",
      "of how significant this possibly was. We returned a couple of years\n",
      "later to watch my first \"Test\" match: South Africa vs. the West\n",
      "Indies. This time we were seated in the correct place, in the Oaks\n",
      "Enclosure, which ironically these days is a grass embankment. I\n",
      "remember the hype and the excitement about this special day. I had\n",
      "seen the Windies in their warmup game against Western Province and I\n",
      "knew that they were something special. They had vicious fast bowlers\n",
      "and flamboyant batsmen. We got there early, along with thousands of\n",
      "others, just to watch the warmups. The excitement was tangible. I\n",
      "don't remember much about the games -- I remember more about the\n",
      "players. They were heroes to us. Perhaps there was some curiosity too,\n",
      "something about the islanders from far, far away on our shores. We\n",
      "used to follow them around the boundaries of the ground, hunting their\n",
      "prized autographs. And their accents -- just too cool. Didn't see\n",
      "color . I know I didn't see the color. I don't think most kids did --\n",
      "they were just cricketers. At the end of play we would run on the\n",
      "field to try to touch them and grab one more autograph. Somewhere\n",
      "there is footage of me congratulating Sylvester Clarke by patting his\n",
      "back. I remember telling all my friends that I had touched the great\n",
      "bowler. While I had forgotten about it, listening to Franklyn\n",
      "Stephenson speak reminded me of the time we were trying to get him to\n",
      "drink from some kid's Coke. He obliged and we all simply went crazy. I\n",
      "realize the significance of that event now. Imagine a black guy\n",
      "drinking from the same can? Perhaps our parents would have flipped\n",
      "out, but this wasn't about color to us. Other memories include how\n",
      "obliging and friendly they were. Happy to give autographs and happy to\n",
      "interact positively with the huge crowds. Branded a rebel . It was\n",
      "with incredible interest and fascination that I watched \"Branded a\n",
      "rebel\" -- a CNN World Sport documentary which first screened last\n",
      "year. It gave me greater insight into the players, the tour, West\n",
      "Indies cricket and indeed my home country. And while \"Branded a rebel\"\n",
      "is an appropriate title for the program, it could have just as easily\n",
      "been titled \"Branded Heroes.\" That's what they were to us white South\n",
      "Africans, and I think deep down the Windies players felt that too.\n",
      "It's all about what side of the line you were sitting, I guess. On\n",
      "their return home, the West Indian players were shunned by society and\n",
      "some were forced to move abroad to make a living. The poverty and\n",
      "other such ills which befell some of their players really tugged at\n",
      "all of my emotions. I started thinking what we as South Africans could\n",
      "have done for them. Could we have done more? Did we look after them\n",
      "well enough after the fact? It's a difficult one. These men didn't\n",
      "deserve it, no matter what choices they made. Society can be cruel.\n",
      "Strangled by hatred . It eats me up to think what those players\n",
      "endured upon their return. They were strangled by such hatred in their\n",
      "home countries, and yet I wish they somehow knew what they had done\n",
      "for young cricket fans like me. I got to see \"real\" international\n",
      "cricket played at the \"highest\" level. I got to meet new cricketing\n",
      "greats. I got to bowl like Clarke, Collis King and Colin Croft. I got\n",
      "to emulate David Murray's great wicketkeeping skills on Clifton beach,\n",
      "field like Alvin Kallicharan and admire the patience and masterful\n",
      "batting of the great Lawrence Rowe. And something else -- they taught\n",
      "us not to take ourselves too seriously by showing how much one can\n",
      "enjoy themselves on the sports field. Their smiles, I'll never forget\n",
      "their smiles. Each of their names is entrenched in my memory, and of\n",
      "all of those who saw the matches. They became part of my upbringing\n",
      "and that of thousands of South African fans. What I didn't understand\n",
      "at the time, was what it took for those men to actually land on our\n",
      "shores (money aside). They must have been scared -- really scared. I\n",
      "know I would have been. In the interview with Clive Lloyd, longtime\n",
      "captain of the official West Indies team that dominated world cricket\n",
      "for two decades, he said no amount of money would have made him\n",
      "support a racist regime. I would go as far as to say no amount of\n",
      "money would have put my fears at ease to travel to South Africa.\n",
      "Courage . It's not for me to judge, but those men were courageous. In\n",
      "their cricket and in their collective decisions, and I respect that. A\n",
      "few years later, I did wonder how the black West Indian players got to\n",
      "travel using \"white\" transport and stay in \"white\" areas. I remember\n",
      "asking my mom how that worked. She didn't know, but you could see that\n",
      "my political curiosity was growing. I was disappointed that more\n",
      "players didn't take up the opportunity of being interviewed for the\n",
      "documentary. I was intrigued by Stephenson, King and Murray's\n",
      "thoughts, motives and reflective answers. There seemed no real regret,\n",
      "which was fascinating and pleasing for some reason. As for those who\n",
      "declined an interview, their wounds obviously cut deep. Stephenson was\n",
      "just how I remember him. Boisterous, bubbly and always looking on the\n",
      "bright side of life. We got to know him well here, as he returned to\n",
      "play for the Orange Free State. The fact that he still has all his\n",
      "memorabilia means that the tour meant something to him, and that he\n",
      "feels part of change and change of attitudes in South Africa. I don't\n",
      "know if they changed us as a society but I'd like to thing so, even if\n",
      "it was an indescribably fractured and dysfunctional society. And by\n",
      "this I mean that white South Africa packed out every cricket ground to\n",
      "watch black West Indies, went home and made sure that \"The Cosby Show\"\n",
      "was the most watched program on television -- and yet black people\n",
      "were pretty much denied and excluded from everything. Heart bleeds .\n",
      "\"Branded a rebel\" showed balanced reporting. I like the fact that you\n",
      "saw the tour from a South African perspective as well. Great players\n",
      "like Graeme Pollock, Barry Richards and Clive Rice also effectively\n",
      "served life bans for being, well, South African, and the program\n",
      "understood that. I know that, fundamentally, the tour was morally\n",
      "corrupt, but for an innocent nine-year-old devoid of political\n",
      "allegiances and beliefs, it was simply a chance to see the world's\n",
      "best cricketers entertaining us on the world's most beautiful stage.\n",
      "Ultimately, I'd also like to believe that South African cricketers and\n",
      "administrators were trying to show the government the way forward, and\n",
      "that unity through sport was possible.<EOS><pad>Sportingban to\n",
      "campaign against apartheid in South Africa introduced in 1977 . \"Rebel\n",
      "tours\" were highly controversial, unofficial, international matches .\n",
      "Team of West Indian cricket players visited South Africa in 1983 .\n",
      "They were shunned by society on their return home to the Caribbean\n",
      ".<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSauPt0NUl_o"
   },
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \n",
    "    depth = query.shape[-1]\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "   \n",
    "    if mask is not None:\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "   \n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis = -1, keepdims = True)\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "    attention = jnp.matmul(dots, value)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        seqlen = x.shape[1]\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        x = jnp.reshape(x, (-1, seqlen, d_head))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    \"\"\"\n",
    "    mask_size = q.shape[1]\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        \"\"\"\n",
    "        seqlen = x.shape[1]\n",
    "        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
    "        x = jnp.transpose(x, ( 0, 2, 1 , 3))\n",
    "\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Adn6DtRWRG"
   },
   "outputs": [],
   "source": [
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer multi-headed causal attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( \n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), \n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), \n",
    "        tl.Dense(d_feature) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKOxnRbp1K5U"
   },
   "outputs": [],
   "source": [
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    feed_forward = [ \n",
    "        tl.LayerNorm(),\n",
    "        tl.Dense(d_ff),\n",
    "        ff_activation(),\n",
    "        tl.Dropout(rate = dropout, mode = mode),\n",
    "        tl.Dense(d_model),\n",
    "        tl.Dropout(rate = dropout, mode = mode)\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          tl.LayerNorm(),\n",
    "          causal_attention,\n",
    "          tl.Dropout(rate = dropout, mode = mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Serial[\n",
      "        Branch_out3[\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "        ]\n",
      "        DotProductAttn_in3\n",
      "        AttnOutput\n",
      "        Dense_512\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Relu\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yi4LJO1RWRS"
   },
   "outputs": [],
   "source": [
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "    \"\"\"\n",
    "    \n",
    "    positional_encoder = [ \n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        tl.Dropout(rate = dropout, mode = mode),\n",
    "        tl.PositionalEncoding(max_len = max_len, mode = mode)]\n",
    "\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.ShiftRight(mode = mode),\n",
    "        positional_encoder,\n",
    "        decoder_blocks,\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        tl.Dense(vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  ShiftRight(1)\n",
      "  Embedding_33300_512\n",
      "  Dropout\n",
      "  PositionalEncoding\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Serial[\n",
      "          Branch_out3[\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "          ]\n",
      "          DotProductAttn_in3\n",
      "          AttnOutput\n",
      "          Dense_512\n",
      "        ]\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Dense_2048\n",
      "        Relu\n",
      "        Dropout\n",
      "        Dense_512\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  LayerNorm\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(TransformerLM(n_layers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)  \n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, \n",
    "      loss_layer=tl.CrossEntropyLoss(), \n",
    "      optimizer=trax.optimizers.Adam(0.01), \n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen,\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] \n",
    "    )\n",
    "\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Ran 1 train steps in 9.16 secs\n",
      "Step      1: train CrossEntropyLoss |  10.41526318\n",
      "Step      1: eval  CrossEntropyLoss |  10.41306114\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 60.16 secs\n",
      "Step     10: train CrossEntropyLoss |  10.41435432\n",
      "Step     10: eval  CrossEntropyLoss |  10.41310978\n",
      "Step     10: eval          Accuracy |  0.00000000\n"
     ]
    }
   ],
   "source": [
    "!rm -f ~/model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "zWoSzR5tkoAx",
    "outputId": "2b9f1cca-4778-4509-bd9e-bd1738625a4e"
   },
   "outputs": [],
   "source": [
    "model = TransformerLM(mode='eval')\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD_bXRCpRWRg"
   },
   "outputs": [],
   "source": [
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "    \"\"\"\n",
    "    token_length = len(cur_output_tokens)\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] \n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HwIdimiN0k2"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(input_sentence, model):\n",
    "\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "9kHuIDGW1sOr",
    "outputId": "2525ca2c-4625-47c0-8456-f75598581993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a sunny day when I went to the market to buy some flowers. But\n",
      "I only found roses, not tulips. \n",
      "\n",
      ":\n",
      ": I\n",
      ": I just\n",
      ": I just found\n",
      ": I just found ros\n",
      ": I just found roses\n",
      ": I just found roses,\n",
      ": I just found roses, not\n",
      ": I just found roses, not tu\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips.\n",
      ": I just found roses, not tulips.<EOS>\n",
      ": I just found roses, not tulips.<EOS>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DYgX-mzjyUia",
    "outputId": "b901e164-48b3-4124-d21a-fe7443d15b79"
   },
   "outputs": [],
   "source": [
    "article = \"Its the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the Tebowing craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Jordan\n",
    "Jordan Ful\n",
    "Jordan Fulcol\n",
    "Jordan Fulcoly\n",
    "Jordan Fulcoly,\n",
    "Jordan Fulcoly, Wayne\n",
    "Jordan Fulcoly, Wayne Dre\n",
    "Jordan Fulcoly, Wayne Drexe\n",
    "Jordan Fulcoly, Wayne Drexel\n",
    "Jordan Fulcoly, Wayne Drexel,\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Final summary:\n",
    "\n",
    "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
    "suspended for one day. Four students were suspended for one day\n",
    "because they allegedly did not heed to warnings that the 'Tebowing'\n",
    "craze was blocking the hallway and presenting a safety hazard to\n",
    "students.<EOS>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-2"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
